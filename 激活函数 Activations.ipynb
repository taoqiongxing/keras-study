{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 激活函数的用法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "激活函数可以通过设置单独的激活层实现，也可以在构造层对象时通过传递activation参数实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Activation, Dense\n",
    "\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('tanh'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 等价于"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(64, activation='tanh'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你也可以通过传递一个逐元素运算的Theano/TensorFlow/CNTK函数来作为激活函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "model.add(Dense(64, activation=K.tanh))\n",
    "model.add(Activation(K.tanh))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 预定义激活函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax(x, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x：张量。\n",
    "\n",
    "axis：整数，代表softmax所作用的维度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 返回\n",
    "\n",
    "softmax变换后的张量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 异常\n",
    "\n",
    "ValueError： In case dim(x) == 1。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## elu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elu(x, alpha=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "指数线性单元。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参数\n",
    "\n",
    "x：张量。\n",
    "\n",
    "alpha：一个标量，表示负数部分的斜率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 返回\n",
    "\n",
    "线性指数激活：如果 x > 0，返回值为 x；如果 x < 0 返回值为 alpha * (exp(x)-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## selu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selu(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可伸缩的指数线性单元（SELU）。\n",
    "\n",
    "SELU 等同于：scale * elu(x, alpha)，其中 alpha 和 scale 是预定义的常量。只要正确初始化权重（参见 lecun_normal 初始化方法）并且输入的数量「足够大」（参见参考文献获得更多信息），选择合适的 alpha 和 scale 的值，就可以在两个连续层之间保留输入的均值和方差。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参数\n",
    "\n",
    "x: 一个用来用于计算激活函数的张量或变量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 返回\n",
    "\n",
    "可伸缩的指数线性激活：scale * elu(x, alpha)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 注意\n",
    "\n",
    "与「lecun_normal」初始化方法一起使用。\n",
    "\n",
    "与 dropout 的变种「AlphaDropout」一起使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## softplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softplus(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softplus 激活函数。\n",
    "\n",
    "### 参数\n",
    "\n",
    "x: 张量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 返回\n",
    "\n",
    "Softsign 激活：x / (abs(x) + 1)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relu(x, alpha=0.0, max_value=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "线性修正单元。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参数\n",
    "\n",
    "x: 张量。\n",
    "\n",
    "alpha：负数部分的斜率。默认为 0。\n",
    "\n",
    "max_value：输出的最大值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 返回\n",
    "\n",
    "线性修正单元激活：如果 x > 0，返回值为 x；如果 x < 0，返回值为 alpha * x。如果定义了 max_value，则结果将截断为此值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tanh(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "双曲正切激活函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigmoid 激活函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hard_sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hard sigmoid 激活函数。\n",
    "\n",
    "计算速度比 sigmoid 激活函数更快。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参数\n",
    "\n",
    "x: 张量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 返回\n",
    "\n",
    "Hard sigmoid 激活：\n",
    "\n",
    "如果 x < -2.5，返回 0。\n",
    "\n",
    "如果 x > 2.5，返回 1。\n",
    "\n",
    "如果 -2.5 <= x <= 2.5，返回 0.2 * x + 0.5。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "线性激活函数（即不做任何改变）\n",
    "\n",
    "### 高级激活函数\n",
    "对于Theano/TensorFlow/CNTK不能表达的复杂激活函数，如含有可学习参数的激活函数，可通过高级激活函数实现，可以在 keras.layers.advanced_activations 模块中找到。 这些高级激活函数包括 PReLU 和 LeakyReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 回调函数使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回调函数是一个函数的合集，会在训练的阶段中所使用。你可以使用回调函数来查看训练模型的内在状态和统计。你可以传递一个列表的回调函数（作为 callbacks 关键字参数）到 Sequential 或 Model 类型的 .fit() 方法。在训练时，相应的回调函数的方法就会被在各自的阶段被调用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.callbacks.Callback()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用来组建新的回调函数的抽象基类。\n",
    "\n",
    "### 属性\n",
    "\n",
    "params: 字典。训练参数， (例如，verbosity, batch size, number of epochs...)。\n",
    "\n",
    "model: keras.models.Model 的实例。 指代被训练模型。\n",
    "\n",
    "被回调函数作为参数的 logs 字典，它会含有于当前批量或训练轮相关数据的键。\n",
    "\n",
    "目前，Sequential 模型类的 .fit() 方法会在传入到回调函数的 logs 里面包含以下的数据：\n",
    "\n",
    "on_epoch_end: 包括 acc 和 loss 的日志， 也可以选择性的包括 val_loss（如果在  fit 中启用验证），和 val_acc（如果启用验证和监测精确值）。\n",
    "\n",
    "on_batch_begin: 包括 size 的日志，在当前批量内的样本数量。\n",
    "\n",
    "on_batch_end: 包括 loss 的日志，也可以选择性的包括 acc（如果启用监测精确值）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BaseLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.callbacks.BaseLogger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "会积累训练轮平均评估的回调函数。\n",
    "\n",
    "这个回调函数被自动应用到每一个 Keras 模型上面。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TerminateOnNaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.callbacks.TerminateOnNaN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当遇到 NaN 损失会停止训练的回调函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ProgbarLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.callbacks.ProgbarLogger(count_mode='samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "会把评估以标准输出打印的回调函数。\n",
    "\n",
    "### 参数\n",
    "\n",
    "count_mode: \"steps\" 或者 \"samples\"。 进度条是否应该计数看见的样本或步骤（批量）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 触发\n",
    "\n",
    "ValueError: 防止不正确的 count_mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.callbacks.History()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把所有事件都记录到 History 对象的回调函数。\n",
    "\n",
    "这个回调函数被自动启用到每一个 Keras 模型。History 对象会被模型的 fit 方法返回。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.callbacks.ModelCheckpoint(filepath, \n",
    "                                monitor='val_loss', \n",
    "                                verbose=0, \n",
    "                                save_best_only=False, \n",
    "                                save_weights_only=False, \n",
    "                                mode='auto', \n",
    "                                period=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在每个训练期之后保存模型。\n",
    "\n",
    "filepath 可以包括命名格式选项，可以由 epoch 的值和 logs 的键（由 on_epoch_end 参数传递）来填充。\n",
    "\n",
    "例如：如果 filepath 是 weights.{epoch:02d}-{val_loss:.2f}.hdf5， 那么模型被保存的的文件名就会有训练轮数和验证损失。\n",
    "\n",
    "### 参数\n",
    "\n",
    "filepath: 字符串，保存模型的路径。\n",
    "\n",
    "monitor: 被监测的数据。\n",
    "\n",
    "verbose: 详细信息模式，0 或者 1 。\n",
    "\n",
    "save_best_only: 如果 save_best_only=True， 被监测数据的最佳模型就不会被覆盖。\n",
    "\n",
    "mode: {auto, min, max} 的其中之一。 如果 save_best_only=True，那么是否覆盖保存文件的决定就取决于被监测数据的最大或者最小值。 对于 val_acc，模式就会是 max，而对于 val_loss，模式就需要是 min，等等。 在 auto 模式中，方向会自动从被监测的数据的名字中判断出来。\n",
    "\n",
    "save_weights_only: 如果 True，那么只有模型的权重会被保存 (model.save_weights(filepath))， 否则的话，整个模型会被保存 (model.save(filepath))。\n",
    "\n",
    "period: 每个检查点之间的间隔（训练轮数）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                              min_delta=0, \n",
    "                              patience=0, \n",
    "                              verbose=0, \n",
    "                              mode='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当被监测的数量不再提升，则停止训练。\n",
    "\n",
    "### 参数\n",
    "\n",
    "monitor: 被监测的数据。\n",
    "\n",
    "min_delta: 在被监测的数据中被认为是提升的最小变化， 例如，小于 min_delta 的绝对变化会被认为没有提升。\n",
    "\n",
    "patience: 没有进步的训练轮数，在这之后训练就会被停止。\n",
    "\n",
    "verbose: 详细信息模式。\n",
    "\n",
    "mode: {auto, min, max} 其中之一。 在 min 模式中， 当被监测的数据停止下降，训练就会停止；在 max 模式中，当被监测的数据停止上升，训练就会停止；在 auto 模式中，方向会自动从被监测的数据的名字中判断出来。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RemoteMonitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.callbacks.RemoteMonitor(root='http://localhost:9000', \n",
    "                              path='/publish/epoch/end/', \n",
    "                              field='data', \n",
    "                              headers=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将事件数据流到服务器的回调函数。\n",
    "\n",
    "需要 requests 库。 事件被默认发送到 root + '/publish/epoch/end/'。 采用 HTTP POST ，其中的 data 参数是以 JSON 编码的事件数据字典。\n",
    "\n",
    "### 参数\n",
    "\n",
    "root: 字符串；目标服务器的根地址。\n",
    "\n",
    "path: 字符串；相对于 root 的路径，事件数据被送达的地址。\n",
    "\n",
    "field: 字符串；JSON ，数据被保存的领域。\n",
    "\n",
    "headers: 字典；可选自定义的 HTTP 的头字段。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LearningRateScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.callbacks.LearningRateScheduler(schedule, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学习速率定时器。\n",
    "\n",
    "### 参数\n",
    "\n",
    "schedule: 一个函数，接受轮索引数作为输入（整数，从 0 开始迭代） 然后返回一个学习速率作为输出（浮点数）。\n",
    "verbose: 整数。 0：安静，1：更新信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.callbacks.TensorBoard(log_dir='./logs', \n",
    "                            histogram_freq=0, \n",
    "                            batch_size=32, \n",
    "                            write_graph=True, \n",
    "                            write_grads=False, \n",
    "                            write_images=False, \n",
    "                            embeddings_freq=0, \n",
    "                            embeddings_layer_names=None, \n",
    "                            embeddings_metadata=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorboard 基本可视化。\n",
    "\n",
    "TensorBoard 是由 Tensorflow 提供的一个可视化工具。\n",
    "\n",
    "这个回调函数为 Tensorboard 编写一个日志， 这样你可以可视化测试和训练的标准评估的动态图像， 也可以可视化模型中不同层的激活值直方图。\n",
    "\n",
    "如果你已经使用 pip 安装了 Tensorflow，你应该可以从命令行启动 Tensorflow："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard --logdir=/full_path_to_your_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参数\n",
    "\n",
    "log_dir: 用来保存被 TensorBoard 分析的日志文件的文件名。\n",
    "\n",
    "histogram_freq: 对于模型中各个层计算激活值和模型权重直方图的频率（训练轮数中）。 如果设置成 0 ，直方图不会被计算。对于直方图可视化的验证数据（或分离数据）一定要明确的指出。\n",
    "\n",
    "write_graph: 是否在 TensorBoard 中可视化图像。 如果 write_graph 被设置为 True，日志文件会变得非常大。\n",
    "\n",
    "write_grads: 是否在 TensorBoard 中可视化梯度值直方图。 histogram_freq 必须要大于 0 。\n",
    "\n",
    "batch_size: 用以直方图计算的传入神经元网络输入批的大小。\n",
    "\n",
    "write_images: 是否在 TensorBoard 中将模型权重以图片可视化。\n",
    "\n",
    "embeddings_freq: 被选中的嵌入层会被保存的频率（在训练轮中）。\n",
    "\n",
    "embeddings_layer_names: 一个列表，会被监测层的名字。 如果是 None 或空列表，那么所有的嵌入层都会被监测。\n",
    "\n",
    "embeddings_metadata: 一个字典，对应层的名字到保存有这个嵌入层元数据文件的名字。 查看 详情 关于元数据的数据格式。 以防同样的元数据被用于所用的嵌入层，字符串可以被传入。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.callbacks.ReduceLROnPlateau(monitor='val_loss', \n",
    "                                  factor=0.1, \n",
    "                                  patience=10, \n",
    "                                  verbose=0, \n",
    "                                  mode='auto', \n",
    "                                  epsilon=0.0001, \n",
    "                                  cooldown=0, \n",
    "                                  min_lr=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当标准评估已经停止时，降低学习速率。\n",
    "\n",
    "当学习停止时，模型总是会受益于降低 2-10 倍的学习速率。 这个回调函数监测一个数据并且当这个数据在一定「有耐心」的训练轮之后还没有进步， 那么学习速率就会被降低。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=5, min_lr=0.001)\n",
    "model.fit(X_train, Y_train, callbacks=[reduce_lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参数\n",
    "\n",
    "monitor: 被监测的数据。\n",
    "\n",
    "factor: 学习速率被降低的因数。新的学习速率 = 学习速率 * 因数\n",
    "\n",
    "patience: 没有进步的训练轮数，在这之后训练速率会被降低。\n",
    "\n",
    "verbose: 整数。0：安静，1：更新信息。\n",
    "\n",
    "mode: {auto, min, max} 其中之一。如果是 min 模式，学习速率会被降低如果被监测的数据已经停止下降； 在 max 模式，学习塑料会被降低如果被监测的数据已经停止上升； 在 auto 模式，方向会被从被监测的数据中自动推断出来。\n",
    "\n",
    "epsilon: 对于测量新的最优化的阀值，只关注巨大的改变。\n",
    "\n",
    "cooldown: 在学习速率被降低之后，重新恢复正常操作之前等待的训练轮数量。\n",
    "\n",
    "min_lr: 学习速率的下边界。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSVLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.callbacks.CSVLogger(filename, separator=',', append=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把训练轮结果数据流到 csv 文件的回调函数。\n",
    "\n",
    "支持所有可以被作为字符串表示的值，包括 1D 可迭代数据，例如，np.ndarray。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_logger = CSVLogger('training.log')\n",
    "model.fit(X_train, Y_train, callbacks=[csv_logger])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参数\n",
    "\n",
    "filename: csv 文件的文件名，例如 'run/log.csv'。\n",
    "\n",
    "separator: 用来隔离 csv 文件中元素的字符串。\n",
    "\n",
    "append: True：如果文件存在则增加（可以被用于继续训练）。False：覆盖存在的文件。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LambdaCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.callbacks.LambdaCallback(on_epoch_begin=None, \n",
    "                               on_epoch_end=None, \n",
    "                               on_batch_begin=None, \n",
    "                               on_batch_end=None, \n",
    "                               on_train_begin=None, \n",
    "                               on_train_end=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在训练进行中创建简单，自定义的回调函数的回调函数。\n",
    "\n",
    "这个回调函数和匿名函数在合适的时间被创建。 需要注意的是回调函数要求位置型参数，如下：\n",
    "\n",
    "on_epoch_begin 和 on_epoch_end 要求两个位置型的参数： epoch, logs\n",
    "\n",
    "on_batch_begin 和 on_batch_end 要求两个位置型的参数： batch, logs\n",
    "\n",
    "on_train_begin 和 on_train_end 要求一个位置型的参数： logs\n",
    "\n",
    "### 参数\n",
    "\n",
    "on_epoch_begin: 在每轮开始时被调用。\n",
    "    \n",
    "on_epoch_end: 在每轮结束时被调用。\n",
    "    \n",
    "on_batch_begin: 在每批开始时被调用。\n",
    "    \n",
    "on_batch_end: 在每批结束时被调用。\n",
    "    \n",
    "on_train_begin: 在模型训练开始时被调用。\n",
    "    \n",
    "on_train_end: 在模型训练结束时被调用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在每一个批开始时，打印出批数。\n",
    "batch_print_callback = LambdaCallback(\n",
    "    on_batch_begin=lambda batch,logs: print(batch))\n",
    "\n",
    "# 把训练轮损失数据流到 JSON 格式的文件。文件的内容\n",
    "# 不是完美的 JSON 格式，但是时每一行都是 JSON 对象。\n",
    "import json\n",
    "json_log = open('loss_log.json', mode='wt', buffering=1)\n",
    "json_logging_callback = LambdaCallback(\n",
    "    on_epoch_end=lambda epoch, logs: json_log.write(\n",
    "        json.dumps({'epoch': epoch, 'loss': logs['loss']}) + '\\n'),\n",
    "    on_train_end=lambda logs: json_log.close()\n",
    ")\n",
    "\n",
    "# 在完成模型训练之后，结束一些进程。\n",
    "processes = ...\n",
    "cleanup_callback = LambdaCallback(\n",
    "    on_train_end=lambda logs: [\n",
    "        p.terminate() for p in processes if p.is_alive()])\n",
    "\n",
    "model.fit(...,\n",
    "          callbacks=[batch_print_callback,\n",
    "                     json_logging_callback,\n",
    "                     cleanup_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 创建一个回调函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你可以通过扩展 keras.callbacks.Callback 基类来创建一个自定义的回调函数。 通过类的属性 self.model，回调函数可以获得它所联系的模型。\n",
    "\n",
    "下面是一个简单的例子，在训练时，保存一个列表的批量损失值："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 例: 记录损失历史"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(10, input_dim=784, kernel_initializer='uniform'))\n",
    "\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "history = LossHistory()\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=128, epochs=20, verbose=0, callbacks=[history])\n",
    "\n",
    "print(history.losses)\n",
    "# 输出\n",
    "'''\n",
    "[0.66047596406559383, 0.3547245744908703, ..., 0.25953155204159617, 0.25901699725311789]\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
